{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basic Assignment\n",
    "## 과제 : spam.csv를 활용하여 유의미한 해석을 도출해주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "- 보시면 아시다시피 spam.csv는 라벨이 있는 데이터입니다.\n",
    "- 7주차 주제가 텍스트 기초인만큼 텍스트만 활용하셔도 되고 라벨까지 활용하셔서 모델을 돌려보셔도 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.iloc[5]['v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['FreeMsg',\n",
       " 'Hey',\n",
       " 'there',\n",
       " 'darling',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'been',\n",
       " '3',\n",
       " 'week',\n",
       " \"'s\",\n",
       " 'now',\n",
       " 'and',\n",
       " 'no',\n",
       " 'word',\n",
       " 'back',\n",
       " '!',\n",
       " 'I',\n",
       " \"'d\",\n",
       " 'like',\n",
       " 'some',\n",
       " 'fun',\n",
       " 'you',\n",
       " 'up',\n",
       " 'for',\n",
       " 'it',\n",
       " 'still',\n",
       " '?',\n",
       " 'Tb',\n",
       " 'ok',\n",
       " '!',\n",
       " 'XxX',\n",
       " 'std',\n",
       " 'chgs',\n",
       " 'to',\n",
       " 'send',\n",
       " ',',\n",
       " 'å£1.50',\n",
       " 'to',\n",
       " 'rcv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시 코드 코드\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "word_tokenize(spam.iloc[5]['v2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "- 수업에서 다룬 임베딩 방법에는 One-hot encoding, CBOW, Skip-gram 등이 있었습니다. 다양한 시도와 '비교' 결과를 함께 적어주세요! 파라미터를 조정해가는 과정도 해석에 도움이 될 수 있겠죠 :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBOW embedding\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "cbow = Word2Vec(sentences=spam['v2'].apply(word_tokenize), vector_size=100, window=5, min_count=3, workers=4, sg=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('happy', 0.9989734292030334),\n",
       " ('im', 0.9988011121749878),\n",
       " ('last', 0.9987855553627014),\n",
       " ('today', 0.9987326264381409),\n",
       " ('being', 0.9987283945083618),\n",
       " ('his', 0.9987174868583679),\n",
       " ('many', 0.9986872673034668),\n",
       " ('night', 0.9986778497695923),\n",
       " ('He', 0.998616099357605),\n",
       " ('n', 0.9986094832420349)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.wv.most_similar('fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip-gram embedding\n",
    "skipgram = Word2Vec(sentences=spam['v2'].apply(word_tokenize), vector_size=100, window=5, min_count=3, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feeling', 0.992814302444458),\n",
       " ('little', 0.9901238679885864),\n",
       " ('company', 0.9896319508552551),\n",
       " ('talking', 0.989328920841217),\n",
       " ('pretty', 0.9891798496246338),\n",
       " ('worried', 0.9889180660247803),\n",
       " ('never', 0.9885623455047607),\n",
       " ('wrong', 0.9879772067070007),\n",
       " ('kind', 0.9878979325294495),\n",
       " ('both', 0.9878274202346802)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.wv.most_similar('fun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 본인이 도출해낸 해석을 적어주세요!\n",
    "\n",
    "- 유사도, Wordcloud, 이진 분류 모델, Plot 뭐든 상관없으니 분명하고 인상적인 해석을 적어주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKip gram, Cbow 둘다 유사한 언어나 비슷한 맥락의 단어를 잘 찾아주는 모습을 확인 하ㅏㄹ 수 있었습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
